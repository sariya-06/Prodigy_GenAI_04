{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMrcE6cftZYUgysQDyqYig6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sariya-06/Prodigy_GenAI_04/blob/main/Pix2Pix_CGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2t5yC045yera"
      },
      "outputs": [],
      "source": [
        "!wget http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz\n",
        "!tar -xvf facades.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "metadata": {
        "id": "AN7YeTT1yi5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(image_file):\n",
        "    image = tf.io.read_file(image_file)\n",
        "    image = tf.image.decode_jpeg(image)\n",
        "\n",
        "    w = tf.shape(image)[1] // 2\n",
        "\n",
        "    input_image = image[:, :w, :]\n",
        "    real_image = image[:, w:, :]\n",
        "\n",
        "    input_image = tf.cast(input_image, tf.float32)\n",
        "    real_image = tf.cast(real_image, tf.float32)\n",
        "\n",
        "    return input_image, real_image"
      ],
      "metadata": {
        "id": "jKmsMMOHyirh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"facades/train/\"\n",
        "train_images = os.listdir(PATH)\n",
        "\n",
        "def load_image_train(image_file):\n",
        "    input_image, real_image = load(PATH + image_file)\n",
        "    input_image = tf.image.resize(input_image, [256, 256])\n",
        "    real_image = tf.image.resize(real_image, [256, 256])\n",
        "    return (input_image/127.5 - 1), (real_image/127.5 - 1)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images)\n",
        "train_dataset = train_dataset.map(load_image_train)\n",
        "train_dataset = train_dataset.batch(1)"
      ],
      "metadata": {
        "id": "HeeEyT3vyihB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(\n",
        "        tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                               kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    if apply_batchnorm:\n",
        "        result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(\n",
        "        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                        padding='same',\n",
        "                                        kernel_initializer=initializer,\n",
        "                                        use_bias=False))\n",
        "\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    if apply_dropout:\n",
        "        result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "    result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "7SDguA_nyiUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Generator():\n",
        "    inputs = tf.keras.layers.Input(shape=[256,256,3])\n",
        "\n",
        "    down_stack = [\n",
        "        downsample(64, 4, apply_batchnorm=False),\n",
        "        downsample(128, 4),\n",
        "        downsample(256, 4),\n",
        "        downsample(512, 4),\n",
        "        downsample(512, 4),\n",
        "        downsample(512, 4),\n",
        "        downsample(512, 4),\n",
        "        downsample(512, 4),\n",
        "    ]\n",
        "\n",
        "    up_stack = [\n",
        "        upsample(512, 4, apply_dropout=True),\n",
        "        upsample(512, 4, apply_dropout=True),\n",
        "        upsample(512, 4, apply_dropout=True),\n",
        "        upsample(512, 4),\n",
        "        upsample(256, 4),\n",
        "        upsample(128, 4),\n",
        "        upsample(64, 4),\n",
        "    ]\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    last = tf.keras.layers.Conv2DTranspose(\n",
        "        3, 4,\n",
        "        strides=2,\n",
        "        padding='same',\n",
        "        kernel_initializer=initializer,\n",
        "        activation='tanh')\n",
        "\n",
        "    x = inputs\n",
        "    skips = []\n",
        "\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "    x = last(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "metadata": {
        "id": "qQl_7yt51U-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generator()"
      ],
      "metadata": {
        "id": "eE7fspsq2TZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Discriminator():\n",
        "    inp = tf.keras.layers.Input(shape=[256,256,3])\n",
        "    tar = tf.keras.layers.Input(shape=[256,256,3])\n",
        "\n",
        "    x = tf.keras.layers.Concatenate()([inp, tar])\n",
        "    x = tf.keras.layers.Conv2D(64, 4, strides=2, padding='same')(x)\n",
        "    x = tf.keras.layers.LeakyReLU()(x)\n",
        "    x = tf.keras.layers.Conv2D(1, 4, padding='same')(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inp, tar], outputs=x)\n",
        "\n",
        "discriminator = Discriminator()"
      ],
      "metadata": {
        "id": "WkH8fI0kyiKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "LAMBDA = 100\n",
        "\n",
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "    return total_gen_loss\n",
        "\n",
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "    fake_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "    return real_loss + fake_loss"
      ],
      "metadata": {
        "id": "lDlCmNIiyh_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "metadata": {
        "id": "TTHnF5Cqyhyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(input_image, target):\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        gen_output = generator(input_image, training=True)\n",
        "\n",
        "        disc_real = discriminator([input_image, target], training=True)\n",
        "        disc_fake = discriminator([input_image, gen_output], training=True)\n",
        "\n",
        "        gen_loss = generator_loss(disc_fake, gen_output, target)\n",
        "        disc_loss = discriminator_loss(disc_real, disc_fake)\n",
        "\n",
        "    generator_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))"
      ],
      "metadata": {
        "id": "a0qKVRJvy0w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for input_image, target in train_dataset:\n",
        "        train_step(input_image, target)\n",
        "    print(\"Epoch completed:\", epoch)"
      ],
      "metadata": {
        "id": "1WNhHH4Xy0pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example_input, example_target in train_dataset.take(1):\n",
        "    prediction = generator(example_input, training=True)\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title(\"Input\")\n",
        "    plt.imshow((example_input[0] + 1) / 2)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title(\"Generated\")\n",
        "    plt.imshow((prediction[0] + 1) / 2)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.title(\"Real\")\n",
        "    plt.imshow((example_target[0] + 1) / 2)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ucJOvZKTy5zt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}