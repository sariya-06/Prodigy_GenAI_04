ğŸ— Pix2Pix â€“ Image-to-Image Translation using Conditional GAN
ğŸ“Œ Project Overview

This project implements Pix2Pix (Conditional GAN) for image-to-image translation.
The model learns to translate a building photo into its corresponding semantic segmentation map.

The architecture consists of:

Generator â†’ U-Net architecture

Discriminator â†’ PatchGAN

Loss Function â†’ Adversarial Loss + L1 Loss

ğŸ§  Model Architecture
ğŸ”¹ Generator (U-Net)

Encoder-Decoder structure

Skip connections for better feature transfer

Captures both low-level and high-level features

ğŸ”¹ Discriminator (PatchGAN)

Classifies image patches instead of full image

Helps generate sharper and realistic outputs

ğŸ“‚ Dataset

Used a paired dataset (Input | Target format)

Each training image contains:

Left side â†’ Input Image

Right side â†’ Target Image

Example:
Building Photo â Segmentation Map

âš™ Training Details

Epochs: 50+

Optimizer: Adam

Batch Size: 1

Image Size: 256Ã—256

Framework: TensorFlow

ğŸ“Š Results

The model successfully generates segmentation maps that closely resemble ground truth images.

Comparison includes:

Input Image

Generated Output

Real Target Image

ğŸš€ What I Learned

Conditional GAN working mechanism

Generator vs Discriminator training

U-Net architecture

PatchGAN concept

Adversarial + L1 Loss balancing

Image preprocessing pipeline

ğŸ›  Technologies Used

Python

TensorFlow

NumPy

Matplotlib

ğŸ“Œ Future Improvements

Train for more epochs

Use larger dataset

Experiment with other GAN variants
